from datetime import timedelta, datetime
import json
import os
from sys import argv
from typing import List, Dict, Any
import pandas as pd
import glob
import logging

# Importación de funciones ETL genéricas personalizadas
import funciones_genericas_etl as generic  # type: ignore

# ========================
# Configuración global
# ========================

DIRECTORIO_DEFECTO = os.path.dirname(os.path.abspath(__file__))

# Configuración del Logger
def setup_logger(date_str: str):
    """
    Configura un sistema de logging robusto que escribe en consola y en archivo.
    """
    log_filename = f"etl_log_{date_str}.log"
    
    # Crear logger personalizado
    logger = logging.getLogger("ETL_Logger")
    logger.setLevel(logging.DEBUG)
    
    # Evitar duplicar handlers si la función se llama múltiples veces
    if logger.hasHandlers():
        logger.handlers.clear()

    # Formato del log: Timestamp - Nivel - Mensaje
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    # Handler de Archivo
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)

    # Handler de Consola
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# Inicializamos una variable global para el logger (se configurará en el main)
logger = logging.getLogger("ETL_Logger")

# ========================
# Funciones utilitarias
# ========================

def leer_archivos(directorio: str, prefijo: str, extension: str) -> List[str]:
    try:
        archivos = glob.glob(f"{directorio}/*{prefijo}*{extension}")
        logger.info(f"Archivos encontrados para prefijo '{prefijo}': {len(archivos)}")
        return archivos
    except Exception as e:
        logger.error(f"No se pudieron listar archivos en {directorio}: {e}", exc_info=True)
        return []

def cargar_datos_en_dataframe(directorio: str, archivos: List[str], extension: str, skiprows: int, separator: str, encoding: str, sheet_name: str, has_headers: bool = True, manual_headers: List[str] = None) -> pd.DataFrame:
    dataframes = []
    for archivo in archivos:
        ruta_completa = os.path.join(directorio, archivo)
        try:
            if extension == ".xlsx":
                df = pd.read_excel(ruta_completa, engine='openpyxl', skiprows=skiprows, sheet_name=sheet_name, header=0 if has_headers else None)
            elif extension == ".xls":
                df = pd.read_excel(ruta_completa, engine='xlrd', skiprows=skiprows, sheet_name=sheet_name, header=0 if has_headers else None)
            else:
                df = pd.read_csv(ruta_completa, sep=separator, encoding=encoding, header=0 if has_headers else None)
            
            # Si no hay headers, asignar los headers manuales
            if not has_headers and manual_headers:
                df.columns = manual_headers
            
            dataframes.append(df)
            logger.info(f"Archivo cargado exitosamente: {archivo} ({len(df)} filas)")
        except Exception as e:
            logger.error(f"Falló la carga del archivo {archivo}: {e}", exc_info=True)
    if not dataframes:
        logger.warning("No se cargó ningún archivo correctamente.")
        return pd.DataFrame()
    return pd.concat(dataframes, ignore_index=True)

def validar_columnas_archivo_original(df, columnas_requeridas: List[str]):
    faltantes = [col for col in columnas_requeridas if col not in df.columns]
    if faltantes:
        error_msg = f"El archivo original no contiene las columnas requeridas: {', '.join(faltantes)}"
        logger.critical(error_msg)
        raise ValueError(error_msg)

def convertir_fecha_excel(fecha_valor, formato_usuario):
    """
    Convierte fechas de Excel manejando tanto datetime objects como strings
    """
    if pd.isna(fecha_valor) or fecha_valor == '' or str(fecha_valor).strip() == '':
        return None
    
    # Si es string, convertir según el formato del usuario
    formato_map = generic.parse_date_format(formato_usuario)

    # Si ya es un datetime/timestamp (Excel lo convirtió automáticamente)
    if isinstance(fecha_valor, (pd.Timestamp, datetime)):
        return fecha_valor.strftime(formato_map['strptime_format'])
    
    fecha_str = str(fecha_valor).strip()
    
    # Si la fecha string ya contiene timestamp (ej: "2025-05-31 00:00:00")
    if ' ' in fecha_str and len(fecha_str) > 10:
        try:
            fecha_obj = pd.to_datetime(fecha_str)
            return fecha_obj.strftime(formato_map['strptime_format'])
        except Exception as e:
            print(f"[Error] No se pudo parsear timestamp: {e}")
    
    # Convertir según formato especificado por usuario
    if formato_usuario in formato_map['original_format']:
        try:
            fecha_obj = datetime.strptime(fecha_str, formato_map['strptime_format'])
            return fecha_obj.strftime(formato_map['strptime_format'])
        except ValueError as e:
            print(f"[Error] No se pudo convertir la fecha '{fecha_str}' con el formato '{formato_usuario}': {e}")
            return None
    else:
        print(f"[Error] Formato '{formato_usuario}' no reconocido")
        return None

def agregar_columnas_faltantes(df, kpi: str, definiciones, default_data_type_id="16", chain_id_value=None):
    if "item_nbr" in df.columns and "upc" not in df.columns:
        df["upc"] = df["item_nbr"]
    elif "upc" in df.columns and "item_nbr" not in df.columns:
        df["item_nbr"] = df["upc"]
    for def_col in definiciones.get(kpi, []):
        col = def_col["col"]
        if col not in df.columns:
            if col == "data_type_id":
                df[col] = default_data_type_id
            elif col == "chain_id":
                df[col] = chain_id_value
            elif not def_col["nullable"]:
                df[col] = None
    return df

def preparar_df_kpi(kpi_config, definiciones, chain_id):
    archivos = leer_archivos(DIRECTORIO_DEFECTO, kpi_config["file_prefix"], kpi_config["extension"])
    if not archivos:
        logger.error(f"No se encontraron archivos para el KPI '{kpi_config['kpi']}'.")
        raise FileNotFoundError(f"No se encontraron archivos para el KPI '{kpi_config['kpi']}'.")

    has_headers_raw = kpi_config.get("has_headers", True)
    has_headers = has_headers_raw if isinstance(has_headers_raw, bool) else str(has_headers_raw).lower() == "true"
    manual_headers = kpi_config.get("manual_headers", [])
    
    df = cargar_datos_en_dataframe(DIRECTORIO_DEFECTO, archivos, kpi_config["extension"], kpi_config["skiprows"], kpi_config["separator"], kpi_config["encoding"], kpi_config["sheet_name"], has_headers, manual_headers)
    if df.empty:
        logger.error(f"El DataFrame para el KPI '{kpi_config['kpi']}' está vacío después de la carga.")
        raise ValueError(f"El DataFrame para '{kpi_config['kpi']}' está vacío.")

    validar_columnas_archivo_original(df, kpi_config["original_required_columns"])
    df.rename(columns=kpi_config["rename_map"], inplace=True)
    return df

def finalizar_etl_kpi(df, kpi: str, date: int, chain_id: int, definiciones, clean_fields: List[str], default_data_type_id: str = "16", date_format_example: str = "%Y%m%d", extension = None, aurora_engine = None, redshift_engine = None, project_id = None):
    df = agregar_columnas_faltantes(df, kpi, definiciones, default_data_type_id, chain_id)

    if extension == ".xlsx" or extension == ".xls":
        df['date_id'] = df['date_id'].apply(lambda x: convertir_fecha_excel(x, date_format_example))

    # convertir fecha
    if "date_id" in df.columns:
        df = generic.standardize_date_format(df, 'date_id', date_format_example)

    # limpieza de caracteres
    df = generic.remove_special_caracter(df, clean_fields)
    for field in clean_fields:
        if field in df.columns:
            df[field] = df[field].apply(generic.clean_text)

    columnas_ordenadas = [col["col"] for col in sorted(definiciones.get(kpi, []), key=lambda x: x["order"])]
    columnas_salida = [col for col in columnas_ordenadas if col in df.columns]

    df_kpi = df[columnas_salida].copy()
    output_file = f"{kpi}_{date}.txt"
    df_kpi.to_csv(output_file, sep="\t", index=False)
    logger.info(f"Archivo final generado exitosamente: {output_file} con {len(df_kpi)} registros")

# ========================
# Procesamiento por KPI
# ========================

{% for kpi_conf in config.kpis %}
def procesar_{{ kpi_conf.kpi }}(kpi_config, chain_id, date, definiciones, data_type_id, aurora_engine=None, redshift_engine=None, project_id=None):
    try:
        df = preparar_df_kpi(kpi_config, definiciones, chain_id)

        # Aquí va cualquier lógica específica de '{{ kpi_conf.kpi }}'

        finalizar_etl_kpi(df, kpi_config["kpi"], date, chain_id, definiciones, kpi_config["clean_fields"], data_type_id, kpi_config["date_format_example"], kpi_config["extension"], aurora_engine, redshift_engine, project_id)
    except Exception as e:
        print(f"[Error en KPI '{{ kpi_conf.kpi }}']: {e}")
{% endfor %}

def procesar_kpi(kpi_config, chain_id, date, definiciones, data_type_id, aurora_engine=None, redshift_engine=None, project_id=None):
    kpi = kpi_config["kpi"]
    {% for kpi_conf in config.kpis %}
    if kpi == "{{ kpi_conf.kpi }}":
        procesar_{{ kpi_conf.kpi }}(kpi_config, chain_id, date, definiciones, data_type_id, aurora_engine, redshift_engine, project_id)
    {% endfor %}

# ========================
# Punto de entrada
# ========================

def main(chain_id: int, date: int, project_id: int = None):
    # Configurar logging al inicio
    global logger
    logger = setup_logger(str(date))
    
    logger.info(f"Iniciando ejecución ETL. ChainID: {chain_id}, Date: {date}, ProjectID: {project_id}")
    try:
        definiciones = generic.get_kpi_definitions()
        config = {{ config | tojson }}
        
        # Configuración de base de datos
        db_config = config.get("database_config", {})
        db_enabled = str(db_config.get("enabled", "False")).lower() == "true"
        aurora_enabled = str(db_config.get("aurora", "False")).lower() == "true"
        redshift_enabled = str(db_config.get("redshift", "False")).lower() == "true"
        
        aurora_engine = None
        redshift_engine = None
        
        if db_enabled and project_id:
            try:
                if aurora_enabled or redshift_enabled:
                    aurora_engine, redshift_engine, _, _, _, _, _ = generic.get_connections(project_id)
                    if not aurora_enabled:
                        aurora_engine = None
                    if not redshift_enabled:
                        redshift_engine = None

                    logger.info("Conexiones a base de datos establecidas.")
            except Exception as e:
                logger.error(f"Error conectando a BD: {db_err}", exc_info=True)
        
        for kpi_conf in config["kpis"]:
            procesar_kpi(
                kpi_config=kpi_conf,
                chain_id=chain_id,
                date=date,
                definiciones=definiciones,
                data_type_id=config["data_type_id"],
                aurora_engine=aurora_engine,
                redshift_engine=redshift_engine,
                project_id=project_id
            )
    except Exception as e:
        logger.critical(f"Fallo irrecuperable en main: {e}", exc_info=True)

if __name__ == "__main__":
    try:
        if len(argv) < 4:
            print("[Error] Se requieren 3 parámetros: chain_id, date y project_id")
            print("Uso: python script.py <chain_id> <date> <project_id> [otros_parametros...]")
        else:
            chain_id = int(argv[1])
            date = int(argv[2])
            project_id = int(argv[3])
            
            # Los argumentos adicionales quedan disponibles en argv[4:] para uso personalizado
            main(chain_id, date, project_id)
    except ValueError as e:
        print(f"[Error] Los parámetros chain_id, date y project_id deben ser números enteros: {e}")
    except Exception as e:
        print(f"[Error al ejecutar el script] {e}")
